{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "385401d9",
   "metadata": {},
   "source": [
    "# Chapter 4 Training Models\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "A linear model makes a prediction by simply computing a weighted sum of the input features, plus a constant called the *bias term*: \n",
    "\n",
    "$$ \\hat{y}=\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e347f2",
   "metadata": {},
   "source": [
    "In this equation: $\\hat{y}$ is the predicted value, $n$ is the number of features, $x_i$ is the i-th feature value, and $\\theta_j$ is the j-th model parameter (including the bias term $\\theta_0$).\n",
    "\n",
    "In vectorized form:\n",
    "$$ \\hat{y} = h_{\\mathbf{\\theta}}(\\mathbf{x})=\\mathbf{\\theta}\\cdot\\mathbf{x} $$\n",
    "\n",
    "If $\\mathbf{\\theta}$ and $\\mathbf{x}$ are column vectors, then the prediction is $\\hat{y}=\\mathbf{\\theta}^{\\text{T}}\\mathbf{x}$, where $\\mathbf{\\theta}^{\\text{T}}$ is the transpose of $\\mathbf{\\theta}$ and $\\mathbf{\\theta}^{\\text{T}}\\mathbf{x}$ is the matrix multiplication of the two vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb89b2f",
   "metadata": {},
   "source": [
    "In order to train a linear regression model we first need a measure of how well the model fits the training data. The most common performance measure of a regression model is the Root Mean Square Error (RMSE). We need to find the value of $\\mathbf{\\theta}$ that minimizes the RMSE. Usually it's simpler to minimize the mean squared error (MSE) rather than the RMSE.\n",
    "\n",
    "The MSE of a linear regression hypothesis $h_{\\mathbf{\\theta}}$ on a training set $\\mathbf{X}$ is:\n",
    "\n",
    "$$ MSE(\\mathbf{X}, h_{\\mathbf{\\theta}})=\\frac{1}{m}\\sum^{m}_{i=1}\\big(\\mathbf{\\theta}^{\\text{T}}\\mathbf{x}^{(i)}-y^{(i)}\\big)^{2}$$\n",
    "\n",
    "To find the value of $\\mathbf{\\theta}$ that minimizes the cost function, there is a analytical solution known as the *Normal Equation*.\n",
    "\n",
    "$$ \\hat{\\mathbf{\\theta}}=(\\mathbf{X}^{\\text{T}}\\mathbf{X})^{-1}\\mathbf{X}^{\\text{T}}\\mathbf{y} $$\n",
    "\n",
    "There are also iterative methods which we will look at later. \n",
    "\n",
    "Let's use the normal equation to perform regression on some test data in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5df7e9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 2.0, 0.0, 15.0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.axis([0, 2, 0, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30d62b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.54914635]\n",
      " [1.         0.083206  ]\n",
      " [1.         1.10742822]\n",
      " [1.         0.64382963]\n",
      " [1.         0.03301831]]\n"
     ]
    }
   ],
   "source": [
    "X_b = np.c_[np.ones((100, 1)), X]  # adds x0 = 1 to each set of x features\n",
    "print(X_b[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea2b162d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.39189079],\n",
       "       [2.58590122]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e698a565",
   "metadata": {},
   "source": [
    "Note that function that we just used to generate the data is $y=4+3x_{1}+\\text{Gaussian noise}$. Now we can make predictions using $\\hat{\\mathbf{\\theta}}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7862a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.39189079],\n",
       "       [9.56369322]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22cd32d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_new, y_predict, 'r-')\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f750780c",
   "metadata": {},
   "source": [
    "We can also perform linear regression with Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f46ba5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.39189079]), array([[2.58590122]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e860bc1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.39189079],\n",
       "       [9.56369322]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d29cccd",
   "metadata": {},
   "source": [
    "The `LinearRegression` class is actually based on the `scipy.linalg.lstsq()` function. We can call that directly too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68831127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.39189079],\n",
       "       [2.58590122]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
    "theta_best_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc21901",
   "metadata": {},
   "source": [
    "The `scipy.linalg.lstsq()` function computes $\\hat{\\mathbf{\\theta}}=\\mathbf{X}^{+}\\mathbf{y}$, where $\\mathbf{X}^{+}$ is the *pseudoinverse* of $\\mathbf{X}$. We can use `np.linalg.pinv()` to compute the pseudoinverse directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d644f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.39189079],\n",
       "       [2.58590122]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(X_b).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec21a121",
   "metadata": {},
   "source": [
    "The pseudoinverse is computed using a technique called *Singular Value Decomposition (SVD)*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a40d0a5",
   "metadata": {},
   "source": [
    "## Computational Complexity\n",
    "\n",
    "Clearly, the normal equation computes the inverse of $\\mathbf{X}^{\\text{T}}\\mathbf{X}$, but this is an $(n+1)\\times (n+1)$ matrix (where $n$ is the number of features)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab06479e",
   "metadata": {},
   "source": [
    "The computational complexity of inverting such a matrix is typically $O(n^{2.4})$ to $O(n^{3})$. On the other hand, the *Singular Value Decomposition (SVD)* approach used by Scikit-Learn's `LinearRegression` class does it with about $O(n^2)$ complexity. \n",
    "\n",
    ">While both the Normal Equation and SVD approach get very slow with increasing number of features, both are linear with regard to the number of instances in the training set (they are $O(m)$), so they can handle large training sets efficiently, provided they can fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc3fc4e",
   "metadata": {},
   "source": [
    "Making *predictions* with a trained model, however, is very fast (almost linear).\n",
    "\n",
    "When there are a large number of features or too many training instances to fit in memory we have to get creative in order to reduce computational complexity and the training time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2532814",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Gradient descent is an iterative optimization algorithm that works by tweaking parameters and simultaneously minimizing a cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86065d8",
   "metadata": {},
   "source": [
    "How does gradient descent work?: It measures the local gradient of the error function (Calculus vibes) with respect to the parameter vector $\\mathbf{\\theta}$ and changes the parameters in the direction of the descending gradient. Once the gradient reaches zero, the function is minimized.\n",
    "\n",
    "Usually, $\\mathbf{\\theta}$ is populated with random values (*random initialization*). The parameters are tweaked slightly with the goal of decreasing the cost function (MSE, for example) until *convergence*.\n",
    "\n",
    "The size of the tweaks to the parameters $\\mathbf{\\theta}$ is deteremined by the *learning rate* hyperparameter. Getting the right learning rate is a balancing act: too small and the algortihm will take a long time to converge, and too large and the algorithm may diverge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f61ab01",
   "metadata": {},
   "source": [
    "#### Not all cost functions are simple to minimize. \n",
    "\n",
    "They may have local minima which make gradient descent's job a lot harder. The algorithm may converge to a local minimum rather than the global one. Fortunately MSE for a LINEAR REGRESSION model is a *convex function*, meaning that the line segment between any two points on the curve never crosses the curve. This implies that there is only one minimum, and it's a global one. MSE is also a continuous function with a slope that does not change abruptly.\n",
    "\n",
    "All of these facts lead to one thing: gradient descent is guaranteed to approach arbitratiliy close to the global minimum!\n",
    "\n",
    "When using gradient descent, all features should have a similar scale or it will take longer to converge. The `StandardScaler` class in Scikit-Learn will do this for us.\n",
    "\n",
    "Training a model with gradient descent means searching for a combination of parameters within the parameter space that minimizes a cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0e2c75",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c718031",
   "metadata": {},
   "source": [
    "To implement gradient descent we compute the gradient of the cost function with respect to *each* model parameter $\\theta_j$:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\theta_j} MSE(\\mathbf{\\theta})=\\frac{2}{m}\\sum^m_{i=1}\\Big(\\mathbf{\\theta}^T\\mathbf{x}^{(i)}-y^{(i)}\\Big)x_{j}^{(i)}$$\n",
    "\n",
    "Instead of computing this partial derivative for each and every model parameter, we can compute the gradient vector instead:\n",
    "\n",
    "$$ \\nabla_{\\theta}MSE(\\mathbf{\\theta})= \\begin{pmatrix}\n",
    "\\frac{\\partial}{\\partial \\theta_0} MSE(\\mathbf{\\theta})\\\\\n",
    "\\frac{\\partial}{\\partial \\theta_1} MSE(\\mathbf{\\theta})\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\partial}{\\partial \\theta_n} MSE(\\mathbf{\\theta})\n",
    "\\end{pmatrix} = \\frac{2}{m}\\mathbf{X}^{T}(\\mathbf{X}\\mathbf{\\theta}-\\mathbf{y})$$\n",
    "\n",
    ">Notice that this formula involves calculations over the entire training set $\\mathbf{X}$ at each gradient descent step. Because the whole batch of training data is used at every step, this algorithm is very slow on large training sets. However, gradient descent scales well iwth the number of features; training a linear regression model when there are hundreds of thousands of features is still much faster using gradient descent than using the normal equation of singular value decomposition!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a97378",
   "metadata": {},
   "source": [
    "The gradient vector in its current form points in the direction of greatest increase, so to go downhill we just subtract it from $\\mathbf{\\theta}$. Remember that the gradient vector is just a slope though, we need to multiply it by some constant to figure out how big of a step we will take! This is where we introduce the learning rate $\\eta$. We multiply $\\eta$ with the gradient vector to determine the size of the downhill step and then subtract that from $\\theta$, giving us a $\\theta$ that is slightly better than the previous:\n",
    "\n",
    "$$ \\mathbf\\theta^{\\text{next step}}=\\mathbf{\\theta}-\\eta\\nabla_{\\theta}MSE(\\mathbf{\\theta}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64db1218",
   "metadata": {},
   "source": [
    "We can implement gradient descent really quickly in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daf785f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.39189079],\n",
       "       [2.58590122]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta = 0.1 # learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2, 1)  \n",
    "# random initialization of theta, i.e. the parameter space which includes the \n",
    "# slope and intercept of the line we previously looked at\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    gradient = 2/m * X_b.T.dot(X_b.dot(theta)-y)\n",
    "    theta = theta - eta*gradient\n",
    "\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe787e7",
   "metadata": {},
   "source": [
    "This is exactly what the normal equation found!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dc2a12",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde1fee8",
   "metadata": {},
   "source": [
    "The main problem with batch gradient descent is that it uses the entire training set to compoute the gradients at every iteration, making it very slow when we have a large training set.\n",
    "\n",
    "On the other hand, *stochastic* gradient descent picks a random instance in the training set at every step and computes the gradients based on that single instance. This makes the algorithm much faster and able to run on huge training sets.\n",
    "\n",
    "The stochastic nature of the algorithm means that it is less regular--the cost function bounces up and down, decreasing on average, but not every step. Eventually, the algorithm reaches near the minimum but continues to bounce around. The final parameters are then good, but not optimal.\n",
    "\n",
    "However, jumping around with the cost function can help the algorithm exit local minima, so stochastic gradient descent is better at finding the global minimum than batch gradient descent.\n",
    "\n",
    "One solution to the fact that stochastic gradient descent can never settle at the minimum is to gradually reduce the learning rate, $\\eta$. The steps start out large, then get smaller, alllowing the algorithm to settle at the global minimum. The function that determines the learning rate at each iteration is called the *learning schedule*.\n",
    "\n",
    "If the learning rate is reduced too quickly, the algorithm can get stuck in a local minimum. On the other hand, reducing it too slowly may cause the algorithm to jump around the minimum for a long time and we may end up with a suboptimal solution if training is stopped too early. \n",
    "\n",
    "We can implement stochastic gradient descent easily in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adeb20c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.40195492],\n",
       "       [2.61650564]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "t0, t1 = 5, 50  # hyperparameters for the learning schedule\n",
    "\n",
    "def learning_schedule(t0, t1, t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta = np.random.randn(2, 1)  # random initialization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(t0, t1, epoch * m + i)\n",
    "        theta = theta - eta * gradients\n",
    "        \n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f857df",
   "metadata": {},
   "source": [
    "We iterated by rounds of *m* iterations; each round is called an epoch. While the batch gradient descent code iterated 1000 times through the whole training set, this code wnet through the training set only 50 times and still got a decent solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4e06a8",
   "metadata": {},
   "source": [
    ">When using stochastic gradient descent, the training instances need to be independent and identically distributed to ensure that the parameters get pulled toward the global optimum, on average. A simple way to ensure this is to shuffle the instances during training. If we don't shuffle the instances, then SGD starts optimizing for one label, then the next, and so on. In doing this it does not settle close to the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f36bbc",
   "metadata": {},
   "source": [
    "To perform linear regression using stochastic gradient descent with Scikit-Learn, we can use the `SGDRegressor` class, which defaults to optimizing the squared error cost function. The following code runs for a maximum of 1000 epochs or until the loss drops by less than 0.001 during one epoch, starting with a learning rate of 0.1, and using the default learning schedule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0fd684c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.42791198]), array([2.61604254]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\n",
    "sgd_reg.fit(X, y.ravel())  # ravel returns a flattened array\n",
    "\n",
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fb731c",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe94a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
